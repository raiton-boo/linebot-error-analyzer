# ğŸ”„ ã‚¨ãƒ©ãƒ¼ãƒ•ãƒ­ãƒ¼åˆ†æã‚¬ã‚¤ãƒ‰

LINE Bot ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ãŠã‘ã‚‹ã‚¨ãƒ©ãƒ¼ã®ç™ºç”Ÿã‹ã‚‰è§£æ±ºã¾ã§ã®ãƒ•ãƒ­ãƒ¼ã‚’ä½“ç³»çš„ã«åˆ†æã—ã€æœ€é©ãªå¯¾å‡¦æ³•ã‚’è¦‹ã¤ã‘ã‚‹ãŸã‚ã®ã‚¬ã‚¤ãƒ‰ã§ã™ã€‚

## ğŸ¯ ã‚¨ãƒ©ãƒ¼ãƒ•ãƒ­ãƒ¼åˆ†æã®æ¦‚è¦

```mermaid
graph TD
    A[LINE API Error] --> B{Error Type Detection}
    B --> C[Client Error 4xx]
    B --> D[Server Error 5xx]
    B --> E[Network Error]
    B --> F[Application Error]

    C --> G[Authentication Analysis]
    C --> H[Rate Limit Analysis]
    C --> I[Request Validation Analysis]

    D --> J[Service Availability Analysis]
    D --> K[Load Analysis]

    E --> L[Connectivity Analysis]
    E --> M[Timeout Analysis]

    F --> N[Code Logic Analysis]
    F --> O[Data Processing Analysis]

    G --> P[Resolution Strategy]
    H --> P
    I --> P
    J --> P
    K --> P
    L --> P
    M --> P
    N --> P
    O --> P

    P --> Q[Action Execution]
    Q --> R[Result Monitoring]
    R --> S{Success?}
    S -->|Yes| T[Complete]
    S -->|No| U[Escalation]
```

## ğŸ” 1. ã‚¨ãƒ©ãƒ¼æ¤œå‡ºãƒ•ãƒ­ãƒ¼

### åŸºæœ¬çš„ãªã‚¨ãƒ©ãƒ¼æ¤œå‡ºãƒ•ãƒ­ãƒ¼

```python
from linebot_error_analyzer import AsyncLineErrorAnalyzer
from line_bot_error_detective.core.models import ErrorCategory, ErrorSeverity
import asyncio
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
import logging

class ErrorFlowAnalyzer:
    """ã‚¨ãƒ©ãƒ¼ãƒ•ãƒ­ãƒ¼åˆ†æå™¨"""

    def __init__(self):
        self.analyzer = AsyncLineErrorAnalyzer()
        self.error_history = []
        self.flow_patterns = {}
        self.logger = logging.getLogger(__name__)

    async def analyze_error_flow(self, error: Dict[str, Any], context: Optional[Dict] = None) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼ãƒ•ãƒ­ãƒ¼ã®åŒ…æ‹¬çš„åˆ†æ"""

        # 1. åŸºæœ¬åˆ†æ
        basic_analysis = await self.analyzer.analyze(error)

        # 2. ãƒ•ãƒ­ãƒ¼åˆ†æ
        flow_analysis = await self._analyze_flow_context(error, context)

        # 3. å±¥æ­´åˆ†æ
        history_analysis = await self._analyze_error_history(error)

        # 4. ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        pattern_analysis = await self._analyze_error_patterns(error)

        # 5. å½±éŸ¿ç¯„å›²åˆ†æ
        impact_analysis = await self._analyze_impact_scope(error, context)

        # 6. è§£æ±ºæˆ¦ç•¥ã®ç”Ÿæˆ
        resolution_strategy = await self._generate_resolution_strategy(
            basic_analysis, flow_analysis, history_analysis, pattern_analysis, impact_analysis
        )

        # åˆ†æçµæœã‚’ã¾ã¨ã‚ã¦è¿”ã™
        flow_result = {
            "timestamp": datetime.now().isoformat(),
            "error_id": self._generate_error_id(error),
            "basic_analysis": basic_analysis.to_dict(),
            "flow_analysis": flow_analysis,
            "history_analysis": history_analysis,
            "pattern_analysis": pattern_analysis,
            "impact_analysis": impact_analysis,
            "resolution_strategy": resolution_strategy,
            "flow_stage": self._determine_flow_stage(basic_analysis),
            "next_actions": self._determine_next_actions(resolution_strategy)
        }

        # å±¥æ­´ã«è¿½åŠ 
        self.error_history.append(flow_result)

        return flow_result

    async def _analyze_flow_context(self, error: Dict[str, Any], context: Optional[Dict]) -> Dict[str, Any]:
        """ãƒ•ãƒ­ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®åˆ†æ"""

        flow_context = {
            "request_context": context.get("request", {}) if context else {},
            "user_context": context.get("user", {}) if context else {},
            "system_context": context.get("system", {}) if context else {},
            "timing_context": self._analyze_timing_context(error, context),
            "dependency_context": self._analyze_dependency_context(error, context)
        }

        return flow_context

    def _analyze_timing_context(self, error: Dict[str, Any], context: Optional[Dict]) -> Dict[str, Any]:
        """ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®åˆ†æ"""

        timing = {
            "error_timestamp": error.get("timestamp", datetime.now().isoformat()),
            "request_duration": context.get("request_duration") if context else None,
            "queue_time": context.get("queue_time") if context else None,
            "is_peak_time": self._is_peak_time(),
            "time_since_last_error": self._calculate_time_since_last_error()
        }

        return timing

    def _analyze_dependency_context(self, error: Dict[str, Any], context: Optional[Dict]) -> Dict[str, Any]:
        """ä¾å­˜é–¢ä¿‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®åˆ†æ"""

        dependencies = {
            "line_api_status": "unknown",  # å®Ÿéš›ã«ã¯LINE APIã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯
            "database_status": context.get("database_status") if context else "unknown",
            "external_services": context.get("external_services", []) if context else [],
            "network_conditions": context.get("network", {}) if context else {}
        }

        return dependencies

    async def _analyze_error_history(self, error: Dict[str, Any]) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼å±¥æ­´ã®åˆ†æ"""

        recent_errors = [
            e for e in self.error_history
            if self._is_recent_error(e, hours=24)
        ]

        similar_errors = [
            e for e in recent_errors
            if self._is_similar_error(error, e)
        ]

        history_analysis = {
            "total_recent_errors": len(recent_errors),
            "similar_error_count": len(similar_errors),
            "error_frequency": self._calculate_error_frequency(similar_errors),
            "escalation_trend": self._analyze_escalation_trend(similar_errors),
            "resolution_history": self._analyze_resolution_history(similar_errors)
        }

        return history_analysis

    async def _analyze_error_patterns(self, error: Dict[str, Any]) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ"""

        error_signature = self._create_error_signature(error)

        if error_signature not in self.flow_patterns:
            self.flow_patterns[error_signature] = {
                "count": 0,
                "first_seen": datetime.now(),
                "last_seen": datetime.now(),
                "resolutions": [],
                "contexts": []
            }

        pattern = self.flow_patterns[error_signature]
        pattern["count"] += 1
        pattern["last_seen"] = datetime.now()

        pattern_analysis = {
            "pattern_signature": error_signature,
            "occurrence_count": pattern["count"],
            "first_occurrence": pattern["first_seen"].isoformat(),
            "time_span": (pattern["last_seen"] - pattern["first_seen"]).total_seconds(),
            "frequency": pattern["count"] / max(1, (datetime.now() - pattern["first_seen"]).days),
            "successful_resolutions": len([r for r in pattern["resolutions"] if r.get("success")]),
            "common_contexts": self._extract_common_contexts(pattern["contexts"])
        }

        return pattern_analysis

    async def _analyze_impact_scope(self, error: Dict[str, Any], context: Optional[Dict]) -> Dict[str, Any]:
        """å½±éŸ¿ç¯„å›²ã®åˆ†æ"""

        impact_scope = {
            "user_impact": self._assess_user_impact(error, context),
            "service_impact": self._assess_service_impact(error, context),
            "business_impact": self._assess_business_impact(error, context),
            "technical_impact": self._assess_technical_impact(error, context),
            "cascade_risk": self._assess_cascade_risk(error, context)
        }

        return impact_scope

    def _assess_user_impact(self, error: Dict[str, Any], context: Optional[Dict]) -> Dict[str, Any]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼å½±éŸ¿ã®è©•ä¾¡"""

        status_code = error.get("status_code")
        error_code = error.get("error_code")

        impact_level = "low"
        affected_features = []
        user_experience = "minor_degradation"

        if status_code in [401, 403]:
            impact_level = "high"
            affected_features = ["authentication", "messaging"]
            user_experience = "feature_unavailable"
        elif status_code == 429:
            impact_level = "medium"
            affected_features = ["messaging_frequency"]
            user_experience = "rate_limited"
        elif status_code >= 500:
            impact_level = "high"
            affected_features = ["all_features"]
            user_experience = "service_unavailable"

        return {
            "impact_level": impact_level,
            "affected_features": affected_features,
            "user_experience": user_experience,
            "estimated_affected_users": context.get("user_count", 0) if context else 0
        }

    def _assess_service_impact(self, error: Dict[str, Any], context: Optional[Dict]) -> Dict[str, Any]:
        """ã‚µãƒ¼ãƒ“ã‚¹å½±éŸ¿ã®è©•ä¾¡"""

        status_code = error.get("status_code")

        availability_impact = "none"
        performance_impact = "none"
        data_integrity_impact = "none"

        if status_code >= 500:
            availability_impact = "high"
            performance_impact = "high"
        elif status_code == 429:
            performance_impact = "medium"
        elif status_code in [400, 422]:
            data_integrity_impact = "low"

        return {
            "availability_impact": availability_impact,
            "performance_impact": performance_impact,
            "data_integrity_impact": data_integrity_impact,
            "service_degradation": self._calculate_service_degradation(error, context)
        }

    async def _generate_resolution_strategy(self, basic_analysis, flow_analysis,
                                           history_analysis, pattern_analysis, impact_analysis) -> Dict[str, Any]:
        """è§£æ±ºæˆ¦ç•¥ã®ç”Ÿæˆ"""

        strategy = {
            "immediate_actions": [],
            "short_term_actions": [],
            "long_term_actions": [],
            "monitoring_requirements": [],
            "escalation_conditions": [],
            "success_criteria": {},
            "estimated_resolution_time": None,
            "confidence_score": 0.0
        }

        # åŸºæœ¬åˆ†æã«åŸºã¥ãå³åº§ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        category = basic_analysis.category
        severity = basic_analysis.severity

        if category == ErrorCategory.AUTH_ERROR:
            strategy["immediate_actions"].extend([
                "verify_channel_access_token",
                "check_token_expiration",
                "validate_webhook_signature"
            ])
            strategy["short_term_actions"].append("implement_token_refresh_mechanism")

        elif category == ErrorCategory.RATE_LIMIT_ERROR:
            strategy["immediate_actions"].extend([
                "implement_exponential_backoff",
                "reduce_request_frequency"
            ])
            strategy["short_term_actions"].append("implement_request_queuing")
            strategy["long_term_actions"].append("optimize_api_usage_patterns")

        elif category == ErrorCategory.VALIDATION_ERROR:
            strategy["immediate_actions"].extend([
                "validate_request_payload",
                "check_message_format",
                "verify_required_fields"
            ])
            strategy["short_term_actions"].append("implement_robust_input_validation")

        elif category == ErrorCategory.SERVER_ERROR:
            strategy["immediate_actions"].extend([
                "implement_retry_with_backoff",
                "check_line_api_status"
            ])
            strategy["escalation_conditions"].append("if_error_persists_over_10_minutes")

        # å±¥æ­´åˆ†æã«åŸºã¥ãæˆ¦ç•¥èª¿æ•´
        if history_analysis["similar_error_count"] > 5:
            strategy["immediate_actions"].append("investigate_root_cause")
            strategy["escalation_conditions"].append("recurring_error_pattern_detected")

        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã«åŸºã¥ãæˆ¦ç•¥èª¿æ•´
        if pattern_analysis["successful_resolutions"] > 0:
            strategy["confidence_score"] = min(1.0, pattern_analysis["successful_resolutions"] / 10)
            strategy["estimated_resolution_time"] = self._estimate_resolution_time(pattern_analysis)

        # å½±éŸ¿åˆ†æã«åŸºã¥ãå„ªå…ˆåº¦èª¿æ•´
        if impact_analysis["user_impact"]["impact_level"] == "high":
            strategy["immediate_actions"].insert(0, "notify_incident_response_team")
            strategy["escalation_conditions"].insert(0, "immediate_escalation_required")

        # ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°è¦ä»¶
        strategy["monitoring_requirements"] = [
            "error_rate_monitoring",
            "response_time_monitoring",
            "success_rate_monitoring"
        ]

        if category == ErrorCategory.RATE_LIMIT_ERROR:
            strategy["monitoring_requirements"].append("request_frequency_monitoring")

        # æˆåŠŸåŸºæº–
        strategy["success_criteria"] = {
            "error_resolution": "error_stops_occurring",
            "service_restoration": "normal_response_times_restored",
            "user_impact_mitigation": "user_complaints_reduced",
            "prevention": "similar_errors_prevented"
        }

        return strategy

    def _determine_flow_stage(self, basic_analysis) -> str:
        """ç¾åœ¨ã®ãƒ•ãƒ­ãƒ¼æ®µéšã®åˆ¤å®š"""

        if basic_analysis.severity == ErrorSeverity.CRITICAL:
            return "incident_response"
        elif basic_analysis.severity == ErrorSeverity.HIGH:
            return "immediate_action_required"
        elif basic_analysis.severity == ErrorSeverity.MEDIUM:
            return "investigation_required"
        else:
            return "monitoring_required"

    def _determine_next_actions(self, resolution_strategy: Dict[str, Any]) -> List[str]:
        """æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®æ±ºå®š"""

        next_actions = []

        # å³åº§ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å„ªå…ˆ
        if resolution_strategy["immediate_actions"]:
            next_actions.extend(resolution_strategy["immediate_actions"][:3])  # æœ€åˆã®3ã¤

        # ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ¡ä»¶ã‚’ãƒã‚§ãƒƒã‚¯
        if resolution_strategy["escalation_conditions"]:
            next_actions.append("check_escalation_conditions")

        # ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°é–‹å§‹
        next_actions.append("start_monitoring")

        return next_actions

    # ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰
    def _generate_error_id(self, error: Dict[str, Any]) -> str:
        """ã‚¨ãƒ©ãƒ¼IDã®ç”Ÿæˆ"""
        import hashlib
        error_string = f"{error.get('status_code')}_{error.get('error_code')}_{error.get('message', '')}"
        return hashlib.md5(error_string.encode()).hexdigest()[:8]

    def _is_peak_time(self) -> bool:
        """ãƒ”ãƒ¼ã‚¯æ™‚é–“ã‹ã©ã†ã‹ã®åˆ¤å®š"""
        current_hour = datetime.now().hour
        return 9 <= current_hour <= 12 or 18 <= current_hour <= 22

    def _calculate_time_since_last_error(self) -> Optional[float]:
        """æœ€å¾Œã®ã‚¨ãƒ©ãƒ¼ã‹ã‚‰ã®æ™‚é–“ã‚’è¨ˆç®—"""
        if not self.error_history:
            return None

        last_error = self.error_history[-1]
        last_time = datetime.fromisoformat(last_error["timestamp"])
        return (datetime.now() - last_time).total_seconds()

    def _is_recent_error(self, error_entry: Dict, hours: int = 24) -> bool:
        """æœ€è¿‘ã®ã‚¨ãƒ©ãƒ¼ã‹ã©ã†ã‹ã®åˆ¤å®š"""
        error_time = datetime.fromisoformat(error_entry["timestamp"])
        return (datetime.now() - error_time) < timedelta(hours=hours)

    def _is_similar_error(self, current_error: Dict, historical_error: Dict) -> bool:
        """é¡ä¼¼ã‚¨ãƒ©ãƒ¼ã‹ã©ã†ã‹ã®åˆ¤å®š"""
        current_sig = self._create_error_signature(current_error)
        historical_sig = historical_error.get("basic_analysis", {}).get("category")
        return current_sig == historical_sig

    def _create_error_signature(self, error: Dict[str, Any]) -> str:
        """ã‚¨ãƒ©ãƒ¼ã‚·ã‚°ãƒãƒãƒ£ã®ä½œæˆ"""
        return f"{error.get('status_code')}_{error.get('error_code', 'none')}"

    def _calculate_error_frequency(self, errors: List[Dict]) -> float:
        """ã‚¨ãƒ©ãƒ¼é »åº¦ã®è¨ˆç®—"""
        if len(errors) < 2:
            return 0.0

        time_span = (datetime.now() - datetime.fromisoformat(errors[0]["timestamp"])).total_seconds() / 3600
        return len(errors) / max(1, time_span)  # errors per hour

    def _analyze_escalation_trend(self, errors: List[Dict]) -> str:
        """ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å‚¾å‘ã®åˆ†æ"""
        if len(errors) < 3:
            return "insufficient_data"

        recent_count = len([e for e in errors[-3:]])
        older_count = len([e for e in errors[:-3]])

        if recent_count > older_count * 1.5:
            return "escalating"
        elif recent_count < older_count * 0.5:
            return "de_escalating"
        else:
            return "stable"

    def _analyze_resolution_history(self, errors: List[Dict]) -> Dict[str, Any]:
        """è§£æ±ºå±¥æ­´ã®åˆ†æ"""
        resolved_count = 0
        total_resolution_time = 0

        for error in errors:
            if error.get("resolved", False):
                resolved_count += 1
                resolution_time = error.get("resolution_time", 0)
                total_resolution_time += resolution_time

        return {
            "resolution_rate": resolved_count / max(1, len(errors)),
            "average_resolution_time": total_resolution_time / max(1, resolved_count),
            "total_resolved": resolved_count
        }

    def _extract_common_contexts(self, contexts: List[Dict]) -> Dict[str, Any]:
        """å…±é€šã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æŠ½å‡º"""
        # å®Ÿè£…ã¯ç°¡ç•¥åŒ–
        return {"common_patterns": "analysis_needed"}

    def _assess_business_impact(self, error: Dict[str, Any], context: Optional[Dict]) -> Dict[str, Any]:
        """ãƒ“ã‚¸ãƒã‚¹å½±éŸ¿ã®è©•ä¾¡"""
        return {
            "revenue_impact": "low",
            "customer_satisfaction_impact": "medium",
            "reputation_impact": "low"
        }

    def _assess_technical_impact(self, error: Dict[str, Any], context: Optional[Dict]) -> Dict[str, Any]:
        """æŠ€è¡“çš„å½±éŸ¿ã®è©•ä¾¡"""
        return {
            "system_stability": "stable",
            "data_consistency": "maintained",
            "performance_degradation": "minimal"
        }

    def _assess_cascade_risk(self, error: Dict[str, Any], context: Optional[Dict]) -> Dict[str, Any]:
        """ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰éšœå®³ãƒªã‚¹ã‚¯ã®è©•ä¾¡"""
        return {
            "risk_level": "low",
            "potential_affected_systems": [],
            "mitigation_required": False
        }

    def _calculate_service_degradation(self, error: Dict[str, Any], context: Optional[Dict]) -> float:
        """ã‚µãƒ¼ãƒ“ã‚¹åŠ£åŒ–åº¦ã®è¨ˆç®—"""
        return 0.1  # 10%ã®åŠ£åŒ–ã¨ä»®å®š

    def _estimate_resolution_time(self, pattern_analysis: Dict[str, Any]) -> int:
        """è§£æ±ºæ™‚é–“ã®æ¨å®šï¼ˆåˆ†ï¼‰"""
        base_time = 30  # åŸºæœ¬30åˆ†
        if pattern_analysis["successful_resolutions"] > 5:
            base_time = 15  # æ—¢çŸ¥ãƒ‘ã‚¿ãƒ¼ãƒ³ãªã‚‰15åˆ†
        return base_time


# 2. é«˜åº¦ãªãƒ•ãƒ­ãƒ¼åˆ†æãƒ„ãƒ¼ãƒ«

class AdvancedFlowAnalyzer:
    """é«˜åº¦ãªã‚¨ãƒ©ãƒ¼ãƒ•ãƒ­ãƒ¼åˆ†æå™¨"""

    def __init__(self):
        self.flow_analyzer = ErrorFlowAnalyzer()
        self.correlation_engine = ErrorCorrelationEngine()
        self.prediction_engine = ErrorPredictionEngine()

    async def comprehensive_flow_analysis(self, error: Dict[str, Any], context: Optional[Dict] = None) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãƒ•ãƒ­ãƒ¼åˆ†æ"""

        # åŸºæœ¬ãƒ•ãƒ­ãƒ¼åˆ†æ
        flow_result = await self.flow_analyzer.analyze_error_flow(error, context)

        # ç›¸é–¢åˆ†æ
        correlation_result = await self.correlation_engine.analyze_correlations(error, context)

        # äºˆæ¸¬åˆ†æ
        prediction_result = await self.prediction_engine.predict_future_errors(error, flow_result)

        # çµ±åˆåˆ†æçµæœ
        comprehensive_result = {
            **flow_result,
            "correlation_analysis": correlation_result,
            "prediction_analysis": prediction_result,
            "risk_assessment": self._assess_comprehensive_risk(flow_result, correlation_result, prediction_result),
            "automated_responses": self._generate_automated_responses(flow_result, correlation_result),
            "learning_insights": self._extract_learning_insights(flow_result, correlation_result, prediction_result)
        }

        return comprehensive_result

    def _assess_comprehensive_risk(self, flow_result: Dict, correlation_result: Dict, prediction_result: Dict) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãƒªã‚¹ã‚¯è©•ä¾¡"""

        base_risk = flow_result["impact_analysis"]["user_impact"]["impact_level"]
        correlation_multiplier = correlation_result.get("risk_multiplier", 1.0)
        prediction_factor = prediction_result.get("risk_factor", 1.0)

        overall_risk_score = self._calculate_risk_score(base_risk) * correlation_multiplier * prediction_factor

        return {
            "overall_risk_score": min(10.0, overall_risk_score),
            "risk_components": {
                "base_risk": base_risk,
                "correlation_impact": correlation_multiplier,
                "prediction_impact": prediction_factor
            },
            "risk_level": self._categorize_risk(overall_risk_score),
            "mitigation_priority": self._determine_mitigation_priority(overall_risk_score)
        }

    def _calculate_risk_score(self, impact_level: str) -> float:
        """ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        risk_scores = {
            "low": 2.0,
            "medium": 5.0,
            "high": 8.0,
            "critical": 10.0
        }
        return risk_scores.get(impact_level, 1.0)

    def _categorize_risk(self, risk_score: float) -> str:
        """ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«ã®åˆ†é¡"""
        if risk_score >= 9.0:
            return "critical"
        elif risk_score >= 7.0:
            return "high"
        elif risk_score >= 4.0:
            return "medium"
        else:
            return "low"

    def _determine_mitigation_priority(self, risk_score: float) -> str:
        """ç·©å’Œå„ªå…ˆåº¦ã®æ±ºå®š"""
        if risk_score >= 8.0:
            return "immediate"
        elif risk_score >= 6.0:
            return "urgent"
        elif risk_score >= 3.0:
            return "normal"
        else:
            return "low"

    def _generate_automated_responses(self, flow_result: Dict, correlation_result: Dict) -> List[Dict[str, Any]]:
        """è‡ªå‹•å¿œç­”ã®ç”Ÿæˆ"""

        automated_responses = []

        # å³åº§ã®è‡ªå‹•å¿œç­”
        for action in flow_result["resolution_strategy"]["immediate_actions"]:
            if action in ["implement_exponential_backoff", "reduce_request_frequency"]:
                automated_responses.append({
                    "type": "auto_throttling",
                    "action": action,
                    "parameters": {"backoff_factor": 2.0, "max_retries": 3},
                    "trigger": "immediate"
                })

        # ç›¸é–¢ãƒ™ãƒ¼ã‚¹ã®å¿œç­”
        if correlation_result.get("correlated_systems"):
            automated_responses.append({
                "type": "system_monitoring",
                "action": "enhance_monitoring",
                "parameters": {"systems": correlation_result["correlated_systems"]},
                "trigger": "correlation_detected"
            })

        return automated_responses

    def _extract_learning_insights(self, flow_result: Dict, correlation_result: Dict, prediction_result: Dict) -> Dict[str, Any]:
        """å­¦ç¿’ã‚¤ãƒ³ã‚µã‚¤ãƒˆã®æŠ½å‡º"""

        return {
            "pattern_learning": {
                "new_patterns_detected": len(prediction_result.get("new_patterns", [])),
                "pattern_evolution": prediction_result.get("pattern_evolution", "stable"),
                "learning_confidence": prediction_result.get("confidence", 0.0)
            },
            "operational_insights": {
                "recurring_issues": flow_result["pattern_analysis"]["occurrence_count"] > 5,
                "resolution_effectiveness": flow_result["history_analysis"]["resolution_history"]["resolution_rate"],
                "improvement_opportunities": self._identify_improvement_opportunities(flow_result)
            },
            "strategic_insights": {
                "system_resilience": correlation_result.get("system_resilience_score", 0.5),
                "prevention_recommendations": self._generate_prevention_recommendations(flow_result, correlation_result),
                "architecture_improvements": self._suggest_architecture_improvements(correlation_result)
            }
        }

    def _identify_improvement_opportunities(self, flow_result: Dict) -> List[str]:
        """æ”¹å–„æ©Ÿä¼šã®ç‰¹å®š"""
        opportunities = []

        if flow_result["history_analysis"]["resolution_history"]["resolution_rate"] < 0.8:
            opportunities.append("improve_error_handling_procedures")

        if flow_result["pattern_analysis"]["occurrence_count"] > 10:
            opportunities.append("implement_proactive_monitoring")

        return opportunities

    def _generate_prevention_recommendations(self, flow_result: Dict, correlation_result: Dict) -> List[str]:
        """äºˆé˜²æ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
        recommendations = []

        # ãƒ•ãƒ­ãƒ¼åˆ†æãƒ™ãƒ¼ã‚¹ã®æ¨å¥¨
        if flow_result["basic_analysis"]["category"] == "RATE_LIMIT_ERROR":
            recommendations.append("implement_adaptive_rate_limiting")

        # ç›¸é–¢åˆ†æãƒ™ãƒ¼ã‚¹ã®æ¨å¥¨
        if correlation_result.get("external_dependency_failures"):
            recommendations.append("implement_circuit_breaker_pattern")

        return recommendations

    def _suggest_architecture_improvements(self, correlation_result: Dict) -> List[str]:
        """ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ”¹å–„ææ¡ˆ"""
        suggestions = []

        if correlation_result.get("single_point_of_failure_detected"):
            suggestions.append("implement_redundancy")

        if correlation_result.get("cascading_failure_risk") > 0.7:
            suggestions.append("implement_bulkhead_isolation")

        return suggestions


class ErrorCorrelationEngine:
    """ã‚¨ãƒ©ãƒ¼ç›¸é–¢åˆ†æã‚¨ãƒ³ã‚¸ãƒ³"""

    async def analyze_correlations(self, error: Dict[str, Any], context: Optional[Dict] = None) -> Dict[str, Any]:
        """ç›¸é–¢åˆ†æã®å®Ÿè¡Œ"""

        return {
            "temporal_correlations": self._analyze_temporal_correlations(error),
            "system_correlations": self._analyze_system_correlations(error, context),
            "user_correlations": self._analyze_user_correlations(error, context),
            "external_correlations": self._analyze_external_correlations(error),
            "risk_multiplier": 1.2,  # ç›¸é–¢ã«ã‚ˆã‚‹è¿½åŠ ãƒªã‚¹ã‚¯
            "correlated_systems": ["database", "cache"],
            "correlation_confidence": 0.8
        }

    def _analyze_temporal_correlations(self, error: Dict[str, Any]) -> Dict[str, Any]:
        """æ™‚é–“çš„ç›¸é–¢ã®åˆ†æ"""
        return {
            "time_pattern": "peak_hours",
            "frequency_correlation": 0.7,
            "seasonal_pattern": None
        }

    def _analyze_system_correlations(self, error: Dict[str, Any], context: Optional[Dict]) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ ç›¸é–¢ã®åˆ†æ"""
        return {
            "dependent_systems": ["line_api", "database"],
            "failure_cascade_risk": 0.3,
            "resource_contention": False
        }

    def _analyze_user_correlations(self, error: Dict[str, Any], context: Optional[Dict]) -> Dict[str, Any]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ç›¸é–¢ã®åˆ†æ"""
        return {
            "affected_user_segments": ["premium_users"],
            "geographic_correlation": None,
            "behavior_correlation": "high_usage_users"
        }

    def _analyze_external_correlations(self, error: Dict[str, Any]) -> Dict[str, Any]:
        """å¤–éƒ¨ç›¸é–¢ã®åˆ†æ"""
        return {
            "third_party_service_status": "degraded",
            "network_conditions": "normal",
            "infrastructure_events": []
        }


class ErrorPredictionEngine:
    """ã‚¨ãƒ©ãƒ¼äºˆæ¸¬ã‚¨ãƒ³ã‚¸ãƒ³"""

    async def predict_future_errors(self, error: Dict[str, Any], flow_result: Dict[str, Any]) -> Dict[str, Any]:
        """å°†æ¥ã®ã‚¨ãƒ©ãƒ¼äºˆæ¸¬"""

        return {
            "recurrence_probability": self._calculate_recurrence_probability(error, flow_result),
            "escalation_probability": self._calculate_escalation_probability(flow_result),
            "cascade_probability": self._calculate_cascade_probability(error, flow_result),
            "time_to_next_occurrence": self._predict_next_occurrence(flow_result),
            "severity_evolution": self._predict_severity_evolution(flow_result),
            "new_patterns": [],
            "pattern_evolution": "stable",
            "confidence": 0.75,
            "risk_factor": 1.1
        }

    def _calculate_recurrence_probability(self, error: Dict[str, Any], flow_result: Dict[str, Any]) -> float:
        """å†ç™ºç¢ºç‡ã®è¨ˆç®—"""
        pattern_count = flow_result["pattern_analysis"]["occurrence_count"]
        resolution_rate = flow_result["history_analysis"]["resolution_history"]["resolution_rate"]

        # ç°¡å˜ãªè¨ˆç®—å¼
        recurrence_prob = min(0.9, pattern_count * 0.1 * (1 - resolution_rate))
        return recurrence_prob

    def _calculate_escalation_probability(self, flow_result: Dict[str, Any]) -> float:
        """ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç¢ºç‡ã®è¨ˆç®—"""
        trend = flow_result["history_analysis"]["escalation_trend"]

        if trend == "escalating":
            return 0.8
        elif trend == "stable":
            return 0.3
        else:  # de_escalating
            return 0.1

    def _calculate_cascade_probability(self, error: Dict[str, Any], flow_result: Dict[str, Any]) -> float:
        """ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰éšœå®³ç¢ºç‡ã®è¨ˆç®—"""
        impact_level = flow_result["impact_analysis"]["user_impact"]["impact_level"]

        cascade_probs = {
            "low": 0.1,
            "medium": 0.3,
            "high": 0.6,
            "critical": 0.8
        }

        return cascade_probs.get(impact_level, 0.2)

    def _predict_next_occurrence(self, flow_result: Dict[str, Any]) -> Optional[int]:
        """æ¬¡å›ç™ºç”Ÿæ™‚é–“ã®äºˆæ¸¬ï¼ˆåˆ†ï¼‰"""
        frequency = flow_result["history_analysis"]["error_frequency"]

        if frequency > 0:
            return int(60 / frequency)  # æ™‚é–“é–“éš”ã‚’åˆ†ã§è¿”ã™

        return None

    def _predict_severity_evolution(self, flow_result: Dict[str, Any]) -> str:
        """é‡è¦åº¦é€²åŒ–ã®äºˆæ¸¬"""
        trend = flow_result["history_analysis"]["escalation_trend"]

        if trend == "escalating":
            return "increasing"
        elif trend == "de_escalating":
            return "decreasing"
        else:
            return "stable"


# ä½¿ç”¨ä¾‹ã¨ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

async def demo_error_flow_analysis():
    """ã‚¨ãƒ©ãƒ¼ãƒ•ãƒ­ãƒ¼åˆ†æã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""

    print("ğŸ” ã‚¨ãƒ©ãƒ¼ãƒ•ãƒ­ãƒ¼åˆ†æãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    print("=" * 50)

    # åˆ†æå™¨ã®åˆæœŸåŒ–
    flow_analyzer = ErrorFlowAnalyzer()
    advanced_analyzer = AdvancedFlowAnalyzer()

    # ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼
    test_errors = [
        {
            "status_code": 401,
            "error_code": "40001",
            "message": "Authentication failed",
            "timestamp": datetime.now().isoformat()
        },
        {
            "status_code": 429,
            "error_code": "42901",
            "message": "Too Many Requests",
            "timestamp": datetime.now().isoformat()
        },
        {
            "status_code": 500,
            "message": "Internal Server Error",
            "timestamp": datetime.now().isoformat()
        }
    ]

    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±
    context = {
        "request": {
            "endpoint": "/v2/bot/message/push",
            "method": "POST",
            "duration": 5.2
        },
        "user": {
            "user_id": "user_12345",
            "plan": "premium"
        },
        "system": {
            "load": 0.8,
            "memory_usage": 0.75
        }
    }

    for i, error in enumerate(test_errors, 1):
        print(f"\nğŸ“Š ã‚¨ãƒ©ãƒ¼ #{i} ã®åˆ†æ:")
        print(f"ã‚¨ãƒ©ãƒ¼: {error['status_code']} - {error.get('message', 'No message')}")

        # åŸºæœ¬ãƒ•ãƒ­ãƒ¼åˆ†æ
        flow_result = await flow_analyzer.analyze_error_flow(error, context)

        print(f"\nğŸ¯ ãƒ•ãƒ­ãƒ¼åˆ†æçµæœ:")
        print(f"  ãƒ•ãƒ­ãƒ¼æ®µéš: {flow_result['flow_stage']}")
        print(f"  å½±éŸ¿ãƒ¬ãƒ™ãƒ«: {flow_result['impact_analysis']['user_impact']['impact_level']}")
        print(f"  æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {', '.join(flow_result['next_actions'][:3])}")

        # è§£æ±ºæˆ¦ç•¥
        strategy = flow_result['resolution_strategy']
        print(f"\nğŸ› ï¸ è§£æ±ºæˆ¦ç•¥:")
        print(f"  å³åº§ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {', '.join(strategy['immediate_actions'][:2])}")
        print(f"  æ¨å®šè§£æ±ºæ™‚é–“: {strategy['estimated_resolution_time']} åˆ†")
        print(f"  ä¿¡é ¼åº¦: {strategy['confidence_score']:.2f}")

        # é«˜åº¦ãªåˆ†æï¼ˆæœ€åˆã®ã‚¨ãƒ©ãƒ¼ã®ã¿ï¼‰
        if i == 1:
            print(f"\nğŸš€ é«˜åº¦ãªåˆ†æ:")
            comprehensive_result = await advanced_analyzer.comprehensive_flow_analysis(error, context)

            risk_assessment = comprehensive_result['risk_assessment']
            print(f"  ç·åˆãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢: {risk_assessment['overall_risk_score']:.1f}/10")
            print(f"  ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«: {risk_assessment['risk_level']}")
            print(f"  ç·©å’Œå„ªå…ˆåº¦: {risk_assessment['mitigation_priority']}")

            # è‡ªå‹•å¿œç­”
            auto_responses = comprehensive_result['automated_responses']
            if auto_responses:
                print(f"  è‡ªå‹•å¿œç­”: {len(auto_responses)}ä»¶è¨­å®šæ¸ˆã¿")

            # å­¦ç¿’ã‚¤ãƒ³ã‚µã‚¤ãƒˆ
            learning = comprehensive_result['learning_insights']
            print(f"  æ–°ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º: {learning['pattern_learning']['new_patterns_detected']}ä»¶")
            print(f"  è§£æ±ºåŠ¹ç‡: {learning['operational_insights']['resolution_effectiveness']:.2f}")

    # åˆ†æçµ±è¨ˆ
    print(f"\nğŸ“ˆ åˆ†æçµ±è¨ˆ:")
    stats = flow_analyzer.analyzer.get_analysis_stats()
    print(f"  ç·åˆ†ææ•°: {stats.get('total_analyzed', 0)}")
    print(f"  ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆæ•°: {stats.get('cache_hits', 0)}")
    print(f"  ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {len(flow_analyzer.flow_patterns)}")

# å®Ÿè¡Œ
if __name__ == "__main__":
    asyncio.run(demo_error_flow_analysis())
```

## 3. ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–æˆ¦ç•¥

### A. äºˆé˜²çš„ãƒ•ãƒ­ãƒ¼

```python
class PreventiveFlowStrategy:
    """äºˆé˜²çš„ãƒ•ãƒ­ãƒ¼æˆ¦ç•¥"""

    def __init__(self):
        self.monitoring_thresholds = {
            "error_rate": 0.05,  # 5%
            "response_time": 3.0,  # 3ç§’
            "queue_depth": 100
        }

    async def implement_preventive_measures(self, historical_patterns: Dict) -> Dict[str, Any]:
        """äºˆé˜²ç­–ã®å®Ÿè£…"""

        measures = {
            "proactive_monitoring": self._setup_proactive_monitoring(historical_patterns),
            "circuit_breakers": self._setup_circuit_breakers(historical_patterns),
            "rate_limiting": self._setup_adaptive_rate_limiting(historical_patterns),
            "health_checks": self._setup_health_checks(historical_patterns)
        }

        return measures
```

### B. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æœ€é©åŒ–

```python
class RealTimeOptimizer:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æœ€é©åŒ–å™¨"""

    async def optimize_flow_response(self, current_error: Dict, flow_context: Dict) -> Dict[str, Any]:
        """ãƒ•ãƒ­ãƒ¼å¿œç­”ã®æœ€é©åŒ–"""

        optimization = {
            "priority_adjustment": self._adjust_priority(current_error, flow_context),
            "resource_allocation": self._optimize_resources(flow_context),
            "response_tuning": self._tune_response_parameters(current_error),
            "escalation_optimization": self._optimize_escalation_path(current_error, flow_context)
        }

        return optimization
```

ã“ã®ã‚¨ãƒ©ãƒ¼ãƒ•ãƒ­ãƒ¼åˆ†æã‚¬ã‚¤ãƒ‰ã¯ã€LINE Bot ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã®ã‚¨ãƒ©ãƒ¼å¯¾å¿œã‚’ä½“ç³»åŒ–ã—ã€åŠ¹ç‡çš„ãªå•é¡Œè§£æ±ºã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã®åŒ…æ‹¬çš„ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’æä¾›ã—ã¾ã™ã€‚
